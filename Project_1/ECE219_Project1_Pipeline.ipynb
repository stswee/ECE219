{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "import time\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit, GridSearchCV\n",
        "from sklearn.metrics import roc_curve, auc, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\n",
        "from sklearn.decomposition import TruncatedSVD, NMF\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import re\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "from statsmodels.stats.contingency_tables import mcnemar\n",
        "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set random seeds\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyjhYyhPJeb4",
        "outputId": "d5ace8c2-159d-45da-8f8b-3207f3571c32"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing functions\n",
        "def clean(text) -> str:\n",
        "  \"\"\"\n",
        "  Processes the text based on rules.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  text : str\n",
        "    The string to process.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  str\n",
        "    The processed string.\n",
        "  \"\"\"\n",
        "  text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "  texter = re.sub(r\"<br />\", \" \", text)\n",
        "  texter = re.sub(r\"&quot;\", \"\\\"\",texter)\n",
        "  texter = re.sub('&#39;', \"\\\"\", texter)\n",
        "  texter = re.sub('\\n', \" \", texter)\n",
        "  texter = re.sub(' u ',\" you \", texter)\n",
        "  texter = re.sub('`',\"\", texter)\n",
        "  texter = re.sub(' +', ' ', texter)\n",
        "  texter = re.sub(r\"(!)\\1+\", r\"!\", texter)\n",
        "  texter = re.sub(r\"(\\?)\\1+\", r\"?\", texter)\n",
        "  texter = re.sub('&amp;', 'and', texter)\n",
        "  texter = re.sub('\\r', ' ',texter)\n",
        "  clean = re.compile('<.*?>')\n",
        "  texter = texter.encode('ascii', 'ignore').decode('ascii')\n",
        "  texter = re.sub(clean, '', texter)\n",
        "  if texter == \"\":\n",
        "    texter = \"\"\n",
        "  return texter\n",
        "\n",
        "def remove_numbers(text = str) -> str:\n",
        "  \"\"\"\n",
        "  Removes numbers from the text.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  text : str\n",
        "    The string to remove numbers from.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  str\n",
        "    The string with numbers removed.\n",
        "  \"\"\"\n",
        "\n",
        "  # Remove numbers\n",
        "  return re.sub(r'\\b\\d+\\b', '', text)\n",
        "\n",
        "def remove_punctuation(text = str) -> str:\n",
        "  \"\"\"\n",
        "  Removes punctuation from the text.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  text : str\n",
        "    The string to remove punctuation from.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  str\n",
        "    The string with punctuation removed.\n",
        "  \"\"\"\n",
        "\n",
        "  # Remove punctuation\n",
        "  return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "def lemmatize(text = str) -> str:\n",
        "  \"\"\"\n",
        "  Lemmatizes the text.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  text : str\n",
        "    The string to lemmatize.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  str\n",
        "    The lemmatized string.\n",
        "  \"\"\"\n",
        "\n",
        "  # Helper function for part of speech\n",
        "  def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "  # Initialize lemmatizer\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "  # Get tokens\n",
        "  words = text.split()\n",
        "\n",
        "  # Get part of speech\n",
        "  pos_tags = pos_tag(words)\n",
        "\n",
        "  # Lemmatize each word\n",
        "  words_lemmatize = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
        "\n",
        "  # Join words back together\n",
        "  return \" \".join(words_lemmatize)\n",
        "\n",
        "def stemming(text = str) -> str:\n",
        "  \"\"\"\n",
        "  Stems the text.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  text : str\n",
        "    The string to stem.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  str\n",
        "    The stemmed string.\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize stemmer\n",
        "  stemmer = PorterStemmer()\n",
        "\n",
        "  # Get tokens\n",
        "  words = text.split()\n",
        "\n",
        "  # Stem each word\n",
        "  words_stem = [stemmer.stem(word) for word in words]\n",
        "\n",
        "  # Join words back together\n",
        "  return \" \".join(words_stem)\n",
        "\n",
        "def preprocessing(df = pd.DataFrame, preprocessing_functions = list) -> pd.DataFrame:\n",
        "  \"\"\"\n",
        "  Preprocesses the text in the dataframe.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  df : pd.DataFrame\n",
        "    The dataframe to preprocess.\n",
        "  preprocessing_functions : list\n",
        "    The list of preprocessing functions to apply.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  pd.DataFrame\n",
        "    The preprocessed dataframe.\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize dataframe\n",
        "  df_copy = df.copy()\n",
        "\n",
        "  # Apply preprocessing functions\n",
        "  for preprocessing_function in preprocessing_functions:\n",
        "    df_copy.map(preprocessing_function)\n",
        "\n",
        "  return df_copy"
      ],
      "metadata": {
        "id": "DnRPY96MJtOo"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation functions\n",
        "\n",
        "def classifier_metrics(y_true = list, y_pred = list, model_name = \"Model\"):\n",
        "  \"\"\"\n",
        "  Prints the metrics for a classifier.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  y_true : list\n",
        "    The true labels.\n",
        "  y_pred : list\n",
        "    The predicted labels.\n",
        "  model_name : str\n",
        "    The name of the model.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  None\n",
        "  \"\"\"\n",
        "\n",
        "  # Print metrics\n",
        "  print(model_name)\n",
        "  print(\"Accuracy: \", accuracy_score(y_true, y_pred))\n",
        "  print(\"Precision: \", precision_score(y_true, y_pred, pos_label = 'sports'))\n",
        "  print(\"Recall: \", recall_score(y_true, y_pred, pos_label = 'sports'))\n",
        "  print(\"F1 score: \", f1_score(y_true, y_pred, pos_label = 'sports'))\n",
        "  print()\n",
        "\n",
        "def plot_roc_curve(y_true = list, y_pred_prob = list, model_name = \"Model\"):\n",
        "  \"\"\"\n",
        "  Plots the ROC curve for a binary classifier.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  y_true : list\n",
        "      The true labels.\n",
        "  y_pred_prob : list\n",
        "      The predicted probabilities.\n",
        "  model_name : str\n",
        "      The name of the model.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  None\n",
        "  \"\"\"\n",
        "\n",
        "  # Binarize string labels\n",
        "  lb = LabelBinarizer(pos_label = 1)\n",
        "  y_true = lb.fit_transform(y_true)\n",
        "\n",
        "  # Compute the ROC curve and AUC score\n",
        "  fpr, tpr, thresholds = roc_curve(y_true, y_pred_prob)\n",
        "  roc_auc = auc(fpr, tpr)\n",
        "\n",
        "  # Plot the ROC curve\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  plt.plot(fpr, tpr, color = 'blue', lw = 2, label = f\"{model_name} (AUC = {roc_auc:.2f})\")\n",
        "  plt.plot([0, 1], [0, 1], color = 'black', lw = 2)\n",
        "  plt.xlabel(\"False Positive Rate\")\n",
        "  plt.ylabel(\"True Positive Rate\")\n",
        "  plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
        "  plt.legend(loc = \"lower right\")\n",
        "  plt.grid(alpha = 0.7)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "ET26cqXgKCD6"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline function\n",
        "def pipeline(train = pd.DataFrame, test = pd.DataFrame, preprocessing_functions = tuple, mindf = int, model = str, k = int, classifier = str, regularization = \"L1\") -> float:\n",
        "  \"\"\"\n",
        "  Runs the pipeline for a given set of parameters.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  train : pd.DataFrame\n",
        "    The training set.\n",
        "  test : pd.DataFrame\n",
        "    The testing set.\n",
        "  preprocessing_functions : tuple\n",
        "    The tuple of preprocessing functions to apply.\n",
        "  mindf : int\n",
        "    The minimum document frequency.\n",
        "  model : str\n",
        "    The model to use.\n",
        "  k : int\n",
        "    The number of components to use.\n",
        "  classifier : str\n",
        "    The classifier to use.\n",
        "  regularization : str\n",
        "    The regularization to use. Used only for logistic regression.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  accuracy : float\n",
        "    The accuracy of the model.\n",
        "  best_param : float\n",
        "    The gamma parameter of the model. Used for SVM and logistic regression.\n",
        "  \"\"\"\n",
        "\n",
        "  ### Loading Data\n",
        "  # Preprocessing\n",
        "  preprocessing_functions = list(preprocessing_functions)\n",
        "  X_train = preprocessing(train, preprocessing_functions)\n",
        "  X_test = preprocessing(test, preprocessing_functions)\n",
        "\n",
        "  ### Feature Extraction\n",
        "  # Convert to TF-IDF Matrix\n",
        "  count_vect = CountVectorizer(stop_words = 'english', min_df = mindf)\n",
        "  tfidf_transformer = TfidfTransformer()\n",
        "  X_train_counts = count_vect.fit_transform(X_train['full_text'])\n",
        "  X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "  X_test_counts = count_vect.transform(X_test['full_text'])\n",
        "  X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
        "\n",
        "  ### Dimensionality Reduction\n",
        "  # LSI Model\n",
        "  if model == \"LSI\":\n",
        "    svd = TruncatedSVD(n_components = k, random_state = 42)\n",
        "    X_train_reduced = svd.fit_transform(X_train_tfidf)\n",
        "    X_test_reduced = svd.transform(X_test_tfidf)\n",
        "  # NMF Model\n",
        "  elif model == \"NMF\":\n",
        "    nmf = NMF(n_components = k, init = 'random', random_state = 42)\n",
        "    W_train = nmf.fit_transform(X_train_tfidf)\n",
        "    H = nmf.components_\n",
        "    X_train_reduced = W_train\n",
        "    W_test = nmf.transform(X_test_tfidf)\n",
        "    X_test_reduced = W_test\n",
        "  else:\n",
        "    raise ValueError(\"Invalid model name\")\n",
        "\n",
        "  ### Classifier\n",
        "\n",
        "  # Parameter grid for SVM and Logistic Regression\n",
        "  param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000, 1000000]}\n",
        "  best_param = None # Default best gamma to none\n",
        "\n",
        "  # SVM Model\n",
        "  if classifier == \"SVM\":\n",
        "    # Perform 5-fold cross validation to choose best gamma\n",
        "    model = LinearSVC(random_state = 42)\n",
        "\n",
        "    # Perform grid search to find best parameter\n",
        "    grid_search = GridSearchCV(model, param_grid, cv = 5, scoring = 'accuracy')\n",
        "    grid_search.fit(X_train_reduced, train['root_label'])\n",
        "\n",
        "    # Get best parameter\n",
        "    best_param = grid_search.best_params_['C']\n",
        "\n",
        "    # Fit model using best parameter\n",
        "    best_model = LinearSVC(C = best_param, random_state = 42)\n",
        "    best_model.fit(X_train_reduced, train['root_label'])\n",
        "    y_pred_best = best_model.fit(X_train_reduced, train['root_label']).predict(X_test_reduced)\n",
        "    y_pred_best_svm = y_pred_best # Keep for McNemar's test\n",
        "    y_pred_best_prob = best_model.fit(X_train_reduced, train['root_label']).decision_function(X_test_reduced)\n",
        "\n",
        "    # Get accuracy\n",
        "    accuracy = accuracy_score(test['root_label'], y_pred_best)\n",
        "\n",
        "  # Logistic Regression Model\n",
        "  elif classifier == \"Logistic Regression\":\n",
        "    if regularization == \"L1\":\n",
        "      # L1 regularization\n",
        "      model = LogisticRegression(random_state = 42, penalty = 'l1', solver = 'liblinear')\n",
        "      grid_search = GridSearchCV(model, param_grid, cv = 5, scoring = 'accuracy')\n",
        "      grid_search.fit(X_train_reduced, train['root_label'])\n",
        "\n",
        "      # Extract best parameter\n",
        "      best_param = grid_search.best_params_['C']\n",
        "\n",
        "      # Fit model using best parameter\n",
        "      best_model = LogisticRegression(C = best_param, random_state = 42, penalty = 'l1', solver = 'liblinear')\n",
        "      best_model.fit(X_train_reduced, train['root_label'])\n",
        "      y_pred_best = best_model.fit(X_train_reduced, train['root_label']).predict(X_test_reduced)\n",
        "      y_pred_best_prob = best_model.fit(X_train_reduced, train['root_label']).predict_proba(X_test_reduced)[:, 1]\n",
        "\n",
        "      # Get accuracy\n",
        "      accuracy = accuracy_score(test['root_label'], y_pred_best)\n",
        "\n",
        "    elif regularization == \"L2\":\n",
        "      # L2 regularization\n",
        "      model = LogisticRegression(random_state = 42, penalty = 'l2')\n",
        "      grid_search = GridSearchCV(model, param_grid, cv = 5, scoring = 'accuracy')\n",
        "      grid_search.fit(X_train_reduced, train['root_label'])\n",
        "\n",
        "      # Extract best parameter\n",
        "      best_param = grid_search.best_params_['C']\n",
        "\n",
        "      # Fit model using best parameter\n",
        "      best_model = LogisticRegression(C = best_param, random_state = 42, penalty = 'l2')\n",
        "      best_model.fit(X_train_reduced, train['root_label'])\n",
        "      y_pred_best = best_model.fit(X_train_reduced, train['root_label']).predict(X_test_reduced)\n",
        "      y_pred_best_lr = y_pred_best # Keep for McNemar's test\n",
        "      y_pred_best_prob = best_model.fit(X_train_reduced, train['root_label']).predict_proba(X_test_reduced)[:, 1]\n",
        "\n",
        "      # Get accuracy\n",
        "      accuracy = accuracy_score(test['root_label'], y_pred_best)\n",
        "\n",
        "    else:\n",
        "      raise ValueError(\"Invalid regularization name\")\n",
        "\n",
        "  # Naive Bayes Classifier\n",
        "  elif classifier == \"Naive Bayes\":\n",
        "    model = GaussianNB()\n",
        "    model.fit(X_train_reduced, train['root_label'])\n",
        "    y_pred = model.predict(X_test_reduced)\n",
        "    y_pred_prob = model.predict_proba(X_test_reduced)[:, 1]\n",
        "\n",
        "    # Get accuracy\n",
        "    accuracy = accuracy_score(test['root_label'], y_pred)\n",
        "\n",
        "  return accuracy, best_param"
      ],
      "metadata": {
        "id": "PiqS6nm8KlKB"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Global variables\n",
        "df = pd.read_csv(\"Project1-ClassificationDataset.csv\")\n",
        "training, testing = train_test_split(df[[\"full_text\", \"root_label\"]], test_size = 0.2)\n",
        "\n",
        "# Test arguments\n",
        "train = training\n",
        "test = testing\n",
        "preprocessing_functions = (clean, remove_numbers, remove_punctuation)\n",
        "mindf = 3\n",
        "model = \"LSI\"\n",
        "model = \"NMF\"\n",
        "k = 5\n",
        "# classifier = \"Logistic Regression\"\n",
        "# classifier = \"Logistic Regression\"\n",
        "classifier = \"Naive Bayes\"\n",
        "regularization = \"L1\"\n",
        "# regularization = \"L2\"\n",
        "\n",
        "pipeline(train, test, preprocessing_functions, mindf, model, k, classifier, regularization)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5ADOB60LcIx",
        "outputId": "82db57f9-5b6f-409b-9a6f-f73f5052b563"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8663793103448276, None)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let combinations be keys and accuracies (and regularization if applicable) be values\n",
        "combinations = []\n",
        "accuracy_values = []\n",
        "\n",
        "# Hyperparameters for pipeline comparison\n",
        "# Feature extraction\n",
        "mindfs = [2, 3, 4, 5]\n",
        "preprocessing_functions_list = [(clean, remove_numbers, remove_punctuation, lemmatize), (clean, remove_numbers, remove_punctuation, stemming)]\n",
        "# preprocessing_functions_list = [(clean, remove_numbers, remove_punctuation)]\n",
        "\n",
        "# Dimensionality Reduction\n",
        "models = [\"LSI\", \"NMF\"]\n",
        "components = [5, 30, 100]\n",
        "\n",
        "# Classifier\n",
        "classifiers = [\"SVM\", \"Logistic Regression\"]\n",
        "regularizations = [\"L1\", \"L2\"]\n",
        "\n",
        "for mindf in mindfs:\n",
        "  for preprocessing_functions in preprocessing_functions_list:\n",
        "    for model in models:\n",
        "      for k in components:\n",
        "        for classifier in classifiers:\n",
        "          if classifier == \"Logistic Regression\":\n",
        "            for regularization in regularizations:\n",
        "              combinations.append((mindf, preprocessing_functions, model, k, classifier, regularization))\n",
        "          else:\n",
        "            combinations.append((mindf, preprocessing_functions, model, k, classifier))\n",
        "\n",
        "for combination in tqdm(combinations, desc = \"Processing combinations\"):\n",
        "  mindf = combination[0]\n",
        "  preprocessing_functions = combination[1]\n",
        "  model = combination[2]\n",
        "  k = combination[3]\n",
        "  classifier = combination[4]\n",
        "  if classifier == \"Logistic Regression\":\n",
        "    regularization = combination[5]\n",
        "    accuracy_values.append(pipeline(train, test, preprocessing_functions, mindf, model, k, classifier, regularization))\n",
        "  else:\n",
        "    accuracy_values.append(pipeline(train, test, preprocessing_functions, mindf, model, k, classifier))\n",
        "\n",
        "# Dictionary to store combinations and accuracy/regularization\n",
        "combinations_accuracy = {}\n",
        "\n",
        "# Add key-value pairs dynamically in a loop\n",
        "for key, value in zip(combinations, accuracy_values):\n",
        "    combinations_accuracy[key] = value\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "geAXV4hNM6hA",
        "outputId": "96c55bb2-662e-4b43-a6e0-5c09261cc738"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing combinations:   2%|▏         | 3/144 [06:12<4:51:54, 124.21s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-f6b1d6cb190b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0maccuracy_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessing_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmindf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularization\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0maccuracy_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessing_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmindf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Dictionary to store combinations and accuracy/regularization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-faaad6539cd9>\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(train, test, preprocessing_functions, mindf, model, k, classifier, regularization)\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0;31m# Preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0mpreprocessing_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessing_functions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m   \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessing_functions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m   \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessing_functions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-444be8604cf2>\u001b[0m in \u001b[0;36mpreprocessing\u001b[0;34m(df, preprocessing_functions)\u001b[0m\n\u001b[1;32m    161\u001b[0m   \u001b[0;31m# Apply preprocessing functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mpreprocessing_function\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessing_functions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     \u001b[0mdf_copy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessing_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdf_copy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, na_action, **kwargs)\u001b[0m\n\u001b[1;32m  10466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10468\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"map\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10470\u001b[0m     def applymap(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10372\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10373\u001b[0m         )\n\u001b[0;32m> 10374\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"apply\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10376\u001b[0m     def map(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    914\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m             \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_numba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m                 \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m                     \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36minfer\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m  10464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10465\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10466\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10468\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"map\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-444be8604cf2>\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m   \u001b[0;31m# Get part of speech\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m   \u001b[0mpos_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m   \u001b[0;31m# Lemmatize each word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \"\"\"\n\u001b[1;32m    168\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Maps to the specified tagset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"eng\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens, return_conf, use_tagdict)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m                 \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_conf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_conf\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, features, return_conf)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Do a secondary alphabetic sort, for stability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}