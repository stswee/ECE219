{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1de353d24744465db591a62350102d0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dcd096e217b74197b488505bf3879db0",
              "IPY_MODEL_1d25ae67250d440d837aff73bf2608ff",
              "IPY_MODEL_3004d16d4a7c4686964be4a301854d4a"
            ],
            "layout": "IPY_MODEL_bdfb37666eb94a16a354dbd82158d51d"
          }
        },
        "dcd096e217b74197b488505bf3879db0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1c142ae860d47f48b6231e0fceabc4f",
            "placeholder": "​",
            "style": "IPY_MODEL_3d38a7f2dd5f48e2b4a3eb76f293a584",
            "value": "Dl Completed...: 100%"
          }
        },
        "1d25ae67250d440d837aff73bf2608ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_383f9504974e4f6197bf0e362f7229b6",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_676832d40b4c40b1a57b95944449deba",
            "value": 5
          }
        },
        "3004d16d4a7c4686964be4a301854d4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fe49004d9cd46528ef910e8eaeb0fbe",
            "placeholder": "​",
            "style": "IPY_MODEL_a25da510169b4dafa1c0b7ea14a44bae",
            "value": " 5/5 [00:03&lt;00:00,  1.02 file/s]"
          }
        },
        "bdfb37666eb94a16a354dbd82158d51d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1c142ae860d47f48b6231e0fceabc4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d38a7f2dd5f48e2b4a3eb76f293a584": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "383f9504974e4f6197bf0e362f7229b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "676832d40b4c40b1a57b95944449deba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7fe49004d9cd46528ef910e8eaeb0fbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a25da510169b4dafa1c0b7ea14a44bae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminaries"
      ],
      "metadata": {
        "id": "QWdfFJaYYF2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install umap\n",
        "# !pip uninstall umap\n",
        "!pip install umap-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJMzhHcamG24",
        "outputId": "83f47940-ba1b-4d1c-85a5-6dec694585c7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting umap-learn\n",
            "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.6.1)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.60.0)\n",
            "Collecting pynndescent>=0.5 (from umap-learn)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from umap-learn) (4.67.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\n",
            "Downloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.13 umap-learn-0.5.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fQKi5LFpX5uq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, HDBSCAN\n",
        "from sklearn.metrics import confusion_matrix, adjusted_rand_score, adjusted_mutual_info_score, rand_score, normalized_mutual_info_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.manifold import TSNE\n",
        "import umap.umap_ as umap\n",
        "\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ],
      "metadata": {
        "id": "WAGPyxz2ZVsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "dataset, info = tfds.load(\"tf_flowers\", as_supervised=True, with_info=True)\n",
        "\n",
        "# Split into training and test sets\n",
        "train_dataset = dataset[\"train\"]\n",
        "\n",
        "# Print dataset info\n",
        "print(info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567,
          "referenced_widgets": [
            "1de353d24744465db591a62350102d0d",
            "dcd096e217b74197b488505bf3879db0",
            "1d25ae67250d440d837aff73bf2608ff",
            "3004d16d4a7c4686964be4a301854d4a",
            "bdfb37666eb94a16a354dbd82158d51d",
            "e1c142ae860d47f48b6231e0fceabc4f",
            "3d38a7f2dd5f48e2b4a3eb76f293a584",
            "383f9504974e4f6197bf0e362f7229b6",
            "676832d40b4c40b1a57b95944449deba",
            "7fe49004d9cd46528ef910e8eaeb0fbe",
            "a25da510169b4dafa1c0b7ea14a44bae"
          ]
        },
        "id": "d5vCB1fNZXAG",
        "outputId": "c3a97c6d-85e9-45a8-b57a-002b775311f1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset 218.21 MiB (download: 218.21 MiB, generated: 221.83 MiB, total: 440.05 MiB) to /root/tensorflow_datasets/tf_flowers/3.0.1...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Completed...:   0%|          | 0/5 [00:00<?, ? file/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1de353d24744465db591a62350102d0d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset tf_flowers downloaded and prepared to /root/tensorflow_datasets/tf_flowers/3.0.1. Subsequent calls will reuse this data.\n",
            "tfds.core.DatasetInfo(\n",
            "    name='tf_flowers',\n",
            "    full_name='tf_flowers/3.0.1',\n",
            "    description=\"\"\"\n",
            "    A large set of images of flowers\n",
            "    \"\"\",\n",
            "    homepage='https://www.tensorflow.org/tutorials/load_data/images',\n",
            "    data_dir='/root/tensorflow_datasets/tf_flowers/incomplete.YHJLAA_3.0.1/',\n",
            "    file_format=tfrecord,\n",
            "    download_size=218.21 MiB,\n",
            "    dataset_size=221.83 MiB,\n",
            "    features=FeaturesDict({\n",
            "        'image': Image(shape=(None, None, 3), dtype=uint8),\n",
            "        'label': ClassLabel(shape=(), dtype=int64, num_classes=5),\n",
            "    }),\n",
            "    supervised_keys=('image', 'label'),\n",
            "    disable_shuffling=False,\n",
            "    splits={\n",
            "        'train': <SplitInfo num_examples=3670, num_shards=2>,\n",
            "    },\n",
            "    citation=\"\"\"@ONLINE {tfflowers,\n",
            "    author = \"The TensorFlow Team\",\n",
            "    title = \"Flowers\",\n",
            "    month = \"jan\",\n",
            "    year = \"2019\",\n",
            "    url = \"http://download.tensorflow.org/example_images/flower_photos.tgz\" }\"\"\",\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rm49IDTabkT",
        "outputId": "16da5cda-67b3-4967-975c-1eecb5d00a59"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = './flowers_features_and_labels.npz'\n",
        "\n",
        "if os.path.exists(filename):\n",
        "    file = np.load(filename)\n",
        "    f_all, y_all = file['f_all'], file['y_all']\n",
        "\n",
        "else:\n",
        "    if not os.path.exists('./flower_photos'):\n",
        "        # download the flowers dataset and extract its images\n",
        "        url = 'http://download.tensorflow.org/example_images/flower_photos.tgz'\n",
        "        with open('./flower_photos.tgz', 'wb') as file:\n",
        "            file.write(requests.get(url).content)\n",
        "        with tarfile.open('./flower_photos.tgz') as file:\n",
        "            file.extractall('./')\n",
        "        os.remove('./flower_photos.tgz')\n",
        "\n",
        "    class FeatureExtractor(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "\n",
        "            vgg = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)\n",
        "\n",
        "            # Extract VGG-16 Feature Layers\n",
        "            self.features = list(vgg.features)\n",
        "            self.features = nn.Sequential(*self.features)\n",
        "            # Extract VGG-16 Average Pooling Layer\n",
        "            self.pooling = vgg.avgpool\n",
        "            # Convert the image into one-dimensional vector\n",
        "            self.flatten = nn.Flatten()\n",
        "            # Extract the first part of fully-connected layer from VGG16\n",
        "            self.fc = vgg.classifier[0]\n",
        "\n",
        "        def forward(self, x):\n",
        "            # It will take the input 'x' until it returns the feature vector called 'out'\n",
        "            out = self.features(x)\n",
        "            out = self.pooling(out)\n",
        "            out = self.flatten(out)\n",
        "            out = self.fc(out)\n",
        "            return out\n",
        "\n",
        "    # Initialize the model\n",
        "    assert torch.cuda.is_available()\n",
        "    feature_extractor = FeatureExtractor().cuda().eval()\n",
        "\n",
        "    dataset = datasets.ImageFolder(root='./flower_photos',\n",
        "                                   transform=transforms.Compose([transforms.Resize(224),\n",
        "                                                                 transforms.CenterCrop(224),\n",
        "                                                                 transforms.ToTensor(),\n",
        "                                                                 transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]))\n",
        "    dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    # Extract features and store them on disk\n",
        "    f_all, y_all = np.zeros((0, 4096)), np.zeros((0,))\n",
        "    for x, y in tqdm(dataloader):\n",
        "        with torch.no_grad():\n",
        "            f_all = np.vstack([f_all, feature_extractor(x.cuda()).cpu()])\n",
        "            y_all = np.concatenate([y_all, y])\n",
        "    np.savez(filename, f_all=f_all, y_all=y_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "rxf5w-MAaPSo",
        "outputId": "fa764421-57ba-4332-e34e-bc58a9dc13a5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:09<00:00, 56.2MB/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-406080b5c47f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Initialize the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# assert torch.cuda.is_available()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mfeature_extractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeatureExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     dataset = datasets.ImageFolder(root='./flower_photos',\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \"\"\"\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \"\"\"\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions"
      ],
      "metadata": {
        "id": "dovZVOSjYV5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 19"
      ],
      "metadata": {
        "id": "CzydHs90f9aB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though the VGG network is trained on a different dataset, the way the architecture finds and uses features to classify images is data-agnostic. At a high-level, when VGG performs feature extraction, it looks for image features such as edges and textures. The deeper the network, the more abstract features VGG can detect (e.g. shapes, object vs. background). These features are universal across images. That is, every image has some notion of edges or shapes. This allows VGG to use the same methods for feature extraction on different images, and VGG is initially trained to extract features with the most discrminative power."
      ],
      "metadata": {
        "id": "cKd1teCNf-uL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 20"
      ],
      "metadata": {
        "id": "1r7r0CLXhvqB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the FeatureExtractor class, the code list(vgg.features) is a set of convolutional layers from the VGG model. At each subsequent convolutional layer, the model extracts more abstract features. For example, at the first layer, the model may extract low-level features, such as edges or corners. These features can be thought of as the basic building blocks of an image. Using the output from the first layer as the input for the next layer, the model may extract higher-level features, such as shape or parts of an object. With sufficient layers, the model extracts features at a high enough level such that these features are useful for classification.\n",
        "\n",
        "By using nn.Sequential(), the model is ultimately constructed such that the image data is passed through a set of convolution layers. Average pooling is used for downsampling and smoothing. Next, Flatten is used to convert the features into a one-dimensional vector. Finally, this one-dimensional vector goes through vgg.classifier[0], the first part of the fully-connected layer from VGG16, to be used for classification."
      ],
      "metadata": {
        "id": "9qGRHaLWhwcS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 21"
      ],
      "metadata": {
        "id": "BGsF5vOtYh3w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TA: For determining number of pixels in original image, do we look at the entire dataset and sum up the pixels? Is the number of features the VGG network extracts per image just the shape? (i.e. 4096)"
      ],
      "metadata": {
        "id": "og3SO17vlGam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f_all.shape, y_all.shape)\n",
        "num_features = f_all.shape[1]"
      ],
      "metadata": {
        "id": "Z_pnS-t4YjmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 22"
      ],
      "metadata": {
        "id": "DDblv793k6uJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to determine density or sparsity from 1-d vector? Number of 0's? Or look at 3670 x 4096 matrix?"
      ],
      "metadata": {
        "id": "XsysoDumk8NZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sparsity as the number of 0's divided by the number of elements\n",
        "sparsity = np.count_nonzero(f_all == 0) / f_all.size\n",
        "print(f\"Sparsity: {sparsity}\")"
      ],
      "metadata": {
        "id": "lRXP5Z3IGkb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 23"
      ],
      "metadata": {
        "id": "oOMntm_wlV5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform t-SNE\n",
        "tsne = TSNE(n_components = 2, random_state = 0)\n",
        "X_embedded = tsne.fit_transform(f_all)"
      ],
      "metadata": {
        "id": "42TCnMBXcBfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot results\n",
        "scatter = plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c = y_all, cmap = 'tab10')\n",
        "plt.title('t-SNE Visualization of Flower Dataset')\n",
        "plt.xlabel('Dimension 1')\n",
        "plt.ylabel('Dimension 2')\n",
        "\n",
        "# Create labels for legend\n",
        "unique_labels = np.unique(y_all)\n",
        "legend_labels = [f'Class {label}' for label in unique_labels]\n",
        "\n",
        "# Add legend\n",
        "plt.legend(*scatter.legend_elements(), title = \"Ground Truth\", loc = \"best\")\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1iJ8HnHCFFm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There appears appears to be a lot of overlap between groups 2 and 4. There is some overlap between groups 0 and 1. Otherwise, the groups are reasonably separated from each other based on these two dimensions alone."
      ],
      "metadata": {
        "id": "foWRa-dnHJJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 24"
      ],
      "metadata": {
        "id": "DTFejwMfHo9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Rand Index using\n",
        "def compute_rand_index(feature_matrix: np.ndarray, labels: np.ndarray, method = str, minsamples = None, minclustersize = 5) -> float:\n",
        "  \"\"\"\n",
        "  Computes the Rand Index for a given feature matrix and labels.\n",
        "\n",
        "  Args:\n",
        "    feature_matrix: A numpy array of shape (n_samples, n_features) containing the feature matrix.\n",
        "    labels: A numpy array of shape (n_samples,) containing the ground truth labels.\n",
        "    method: A string representing the clustering method to use. Can be either \"kmeans\" or \"agglomerative\" or \"HDBSCAN\" or \"None.\n",
        "    minsamples: An integer representing the minimum number of samples required to form a cluster. Only used for HDBSCAN.\n",
        "    minclustersize: An integer representing the minimum cluster size. Only used for HDBSCAN.\n",
        "\n",
        "  Returns:\n",
        "    A float representing the Rand Index\n",
        "  \"\"\"\n",
        "\n",
        "  # Number of clusters equal to number of unique labels\n",
        "  n_clusters = len(np.unique(labels))\n",
        "\n",
        "  if (method == \"kmeans\"):\n",
        "    # Perform KMeans\n",
        "    kmeans = KMeans(n_clusters = n_clusters, random_state = 0)\n",
        "    kmeans.fit(feature_matrix)\n",
        "    cluster_labels = kmeans.labels_\n",
        "  elif (method == \"agglomerative\"):\n",
        "    # Perform Agglomerative Clustering\n",
        "    agglomerative = AgglomerativeClustering(n_clusters = n_clusters)\n",
        "    agglomerative.fit(feature_matrix)\n",
        "    cluster_labels = agglomerative.labels_\n",
        "  elif (method == \"HDBSCAN\"):\n",
        "    # Perform HDBSCAN\n",
        "    hdbscan = HDBSCAN(min_samples = minsamples, min_cluster_size = minclustersize, metric = 'manhattan')\n",
        "    hdbscan.fit(feature_matrix)\n",
        "    cluster_labels = hdbscan.labels_\n",
        "  elif (method == \"None\"):\n",
        "    pass\n",
        "  else:\n",
        "    raise ValueError(\"Invalid clustering method. Must be either 'kmeans', 'agglomerative', 'HDBSCAN', or 'None'.\")\n",
        "\n",
        "  # Compute rand index\n",
        "  rand_index = rand_score(labels, cluster_labels)\n",
        "\n",
        "  return rand_index\n"
      ],
      "metadata": {
        "id": "e-Cl3GHOHrrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensionality Reduction\n",
        "# SVD\n",
        "def svd_reduce(X: np.ndarray, n_components: int) -> np.ndarray:\n",
        "  \"\"\"\n",
        "  Performs SVD on a given matrix.\n",
        "\n",
        "  Args:\n",
        "    X: A numpy array of shape (n_samples, n_features) containing the feature\n",
        "    n_components: An integer representing the number of components to keep.\n",
        "\n",
        "  Returns:\n",
        "    A numpy array of shape (n_samples, n_components) containing the reduced feature matrix.\n",
        "  \"\"\"\n",
        "\n",
        "  # Perform truncated SVD\n",
        "  svd = TruncatedSVD(n_components = n_components)\n",
        "  svd.fit(X)\n",
        "  X_reduced = svd.transform(X)\n",
        "\n",
        "  return X_reduced\n",
        "\n",
        "# UMAP\n",
        "def umap_reduce(X: np.ndarray, n_components: int) -> np.ndarray:\n",
        "  \"\"\"\n",
        "  Performs UMAP on a given matrix.\n",
        "\n",
        "  Args:\n",
        "    X: A numpy array of shape (n_samples, n_features) containing the feature matrix.\n",
        "    n_components: An integer representing the number of components to keep.\n",
        "\n",
        "  Returns:\n",
        "    A numpy array of shape (n_samples, n_components) containing the reduced feature matrix.\n",
        "  \"\"\"\n",
        "\n",
        "  # Perform UMAP\n",
        "  umap_reducer = umap.UMAP(n_components = n_components)\n",
        "  X_reduced = umap_reducer.fit_transform(X)\n",
        "\n",
        "  return X_reduced\n",
        "  umap_reducer = umap.UMAP(n_components = n_components)\n",
        "  X_reduced = umap_reducer.fit_transform(X)\n",
        "\n",
        "  return X_reduced"
      ],
      "metadata": {
        "id": "VZOkp7RJUZaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensionality Reduction\n",
        "class Autoencoder(torch.nn.Module, TransformerMixin):\n",
        "    def __init__(self, n_components):\n",
        "        super().__init__()\n",
        "        self.n_components = n_components\n",
        "        self.n_features = None  # to be determined with data\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "\n",
        "    def _create_encoder(self):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(4096, 1280),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(1280, 640),\n",
        "            nn.ReLU(True), nn.Linear(640, 120), nn.ReLU(True), nn.Linear(120, self.n_components))\n",
        "\n",
        "    def _create_decoder(self):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(self.n_components, 120),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(120, 640),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(640, 1280),\n",
        "            nn.ReLU(True), nn.Linear(1280, 4096))\n",
        "\n",
        "    def forward(self, X):\n",
        "        encoded = self.encoder(X)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "    def fit(self, X):\n",
        "        X = torch.tensor(X, dtype=torch.float32, device='cuda')\n",
        "        self.n_features = X.shape[1]\n",
        "        self.encoder = self._create_encoder()\n",
        "        self.decoder = self._create_decoder()\n",
        "        self.cuda()\n",
        "        self.train()\n",
        "\n",
        "        criterion = nn.MSELoss()\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "\n",
        "        dataset = TensorDataset(X)\n",
        "        dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "        for epoch in tqdm(range(100)):\n",
        "            for (X_,) in dataloader:\n",
        "                X_ = X_.cuda()\n",
        "                # ===================forward=====================\n",
        "                output = self(X_)\n",
        "                loss = criterion(output, X_)\n",
        "                # ===================backward====================\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = torch.tensor(X, dtype=torch.float32, device='cuda')\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            return self.encoder(X).cpu().numpy()"
      ],
      "metadata": {
        "id": "P0g7qRD2PtK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create list of modules\n",
        "dimensionality_reduction = [\"None\", \"SVD\", \"UMAP\", \"Autoencoder\"]\n",
        "clutering = [\"KMeans\", \"Agglomerative\", \"HDBSCAN\"]\n",
        "min_cluster_size = [10, 20, 30]\n",
        "min_samples = [5, 10, 15]\n",
        "\n",
        "# Create list of combinations of modules\n",
        "modules = []\n",
        "for dr in dimensionality_reduction:\n",
        "    for c in clutering:\n",
        "        if (c == \"HDBSCAN\"):\n",
        "            for ms in min_samples:\n",
        "                for mcs in min_cluster_size:\n",
        "                    modules.append((dr, c, ms, mcs))\n",
        "        else:\n",
        "            modules.append((dr, c))"
      ],
      "metadata": {
        "id": "LC4PJkGUkdWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KMeans very quick (<2 seconds)\n",
        "# Agglomerative ok (~20 seconds)\n",
        "# HDBSCAN slow (~3 minutes)\n",
        "# compute_rand_index(f_all, y_all, \"HDBSCAN\")\n",
        "\n",
        "# Initialize rand index\n",
        "rand_index_all = []\n",
        "\n",
        "# Compute rand index for each module and keep track of progress\n",
        "\n",
        "for module in tqdm(modules):\n",
        "    print(f\"Computing rand index for {module}\")\n",
        "\n",
        "    # Perform dimensionality reduction\n",
        "    if (module[0] == \"SVD\"):\n",
        "        f_all_reduced = svd_reduce(f_all, n_components = 50)\n",
        "    elif (module[0] == \"UMAP\"):\n",
        "        f_all_reduced = umap_reduce(f_all, n_components = 50)\n",
        "    elif (module[0] == \"Autoencoder\"):\n",
        "        f_all_reduced = Autoencoder(50).fit_transform(f_all)\n",
        "    elif (module[0] == \"None\"):\n",
        "        f_all_reduced = f_all\n",
        "    else:\n",
        "        raise ValueError(\"Invalid dimensionality reduction method. Must be either 'SVD', 'UMAP', or 'None'.\")\n",
        "\n",
        "    # Perform clustering\n",
        "    if (module[1] == \"KMeans\"):\n",
        "        rand_index = compute_rand_index(f_all_reduced, y_all, \"kmeans\")\n",
        "        rand_index_all.append(rand_index)\n",
        "    elif (module[1] == \"Agglomerative\"):\n",
        "        rand_index = compute_rand_index(f_all_reduced, y_all, \"agglomerative\")\n",
        "        rand_index_all.append(rand_index)\n",
        "    elif (module[1] == \"HDBSCAN\"):\n",
        "        rand_index = compute_rand_index(f_all_reduced, y_all, \"HDBSCAN\", module[2], module[3])\n",
        "        rand_index_all.append(rand_index)\n"
      ],
      "metadata": {
        "id": "4eI6iIP0K2cV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dictionary of modules and rand_index\n",
        "results_dict = {}\n",
        "for i in range(len(modules)):\n",
        "    results_dict[modules[i]] = rand_index_all[i]\n",
        "results_dict"
      ],
      "metadata": {
        "id": "mDCi1VVDlufC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get module with highest rand index\n",
        "max_key = max(results_dict, key=results_dict.get)\n",
        "max_key"
      ],
      "metadata": {
        "id": "H3ffiXM4l5ST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 25"
      ],
      "metadata": {
        "id": "lqbCuTdMNuWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(num_features, 1280),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(1280, 640),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(640, 5),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "        self.cuda()\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.model(X)\n",
        "\n",
        "    def train(self, X, y):\n",
        "        X = torch.tensor(X, dtype=torch.float32, device='cuda')\n",
        "        y = torch.tensor(y, dtype=torch.int64, device='cuda')\n",
        "\n",
        "        self.model.train()\n",
        "\n",
        "        criterion = nn.NLLLoss()\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "\n",
        "        dataset = TensorDataset(X, y)\n",
        "        dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "        for epoch in tqdm(range(100)):\n",
        "            for (X_, y_) in dataloader:\n",
        "                optimizer.zero_grad()\n",
        "                loss = criterion(self(X_), y_)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            # Print loss every 10 epochs\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
        "        return self\n",
        "\n",
        "    def eval(self, X_test, y_test):\n",
        "        X_test = torch.tensor(X_test, dtype=torch.float32, device='cuda')\n",
        "        y_test = torch.tensor(y_test, dtype=torch.int64, device='cuda')\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            y_pred = self(X_test)\n",
        "            _, y_pred = torch.max(y_pred, 1)\n",
        "            accuracy = torch.sum(y_pred == y_test).item() / len(y_test)\n",
        "        return accuracy\n"
      ],
      "metadata": {
        "id": "cZurEdyoNvpk"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run MLP classifier (very fast even on original dataset)\n",
        "mlp = MLP(num_features)\n",
        "mlp.train(f_all, y_all)\n",
        "mlp.eval(f_all, y_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1oPrwR_PBEx",
        "outputId": "2e7729c1-1a62-458f-ac1f-c3c04cd4e262"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 3/100 [00:00<00:20,  4.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.4495242238044739\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 13/100 [00:01<00:08,  9.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Loss: 0.17565816640853882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 23%|██▎       | 23/100 [00:02<00:07, 10.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20, Loss: 0.001673591323196888\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 33%|███▎      | 33/100 [00:03<00:06, 10.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30, Loss: 0.00023921734828036278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 43%|████▎     | 43/100 [00:04<00:05, 10.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40, Loss: 0.00013989247963763773\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 53%|█████▎    | 53/100 [00:05<00:04, 10.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50, Loss: 4.728128624265082e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 63%|██████▎   | 63/100 [00:06<00:03, 10.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 60, Loss: 1.429455187462736e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 73%|███████▎  | 73/100 [00:07<00:02, 10.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 70, Loss: 0.018813736736774445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 82%|████████▏ | 82/100 [00:08<00:01,  9.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 80, Loss: 5.4297710448736325e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▏| 92/100 [00:09<00:00,  8.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 90, Loss: 4.0997430915012956e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:10<00:00,  9.88it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9997275204359674"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    }
  ]
}