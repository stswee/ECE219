{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image dataset: https://www.kaggle.com/datasets/hlrhegemony/pokemon-image-dataset?resource=download\n",
    "##### Metainfo csv: https://github.com/lgreski/pokemonData/blob/master/Pokemon.csv\n",
    "##### You can speed up model inference on colab by changing Hardware accelerator in runtime type to any GPU option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets transformers numpy pandas Pillow matplotlib \n",
    "!pip install torch tqdm scipy\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install plotly umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import clip\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from scipy.special import softmax\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv file and image paths to construct pokedex, use type_to_load=None to load all types, else use a list of types 1 to load\n",
    "def construct_pokedex(csv_path='Pokemon.csv', image_dir='./images/', type_to_load=None):\n",
    "    pokedex = pd.read_csv(csv_path)\n",
    "    image_paths = []\n",
    "\n",
    "    for pokemon_name in pokedex[\"Name\"]:\n",
    "        imgs = glob(f\"{image_dir}/{pokemon_name}/0.jpg\")\n",
    "        if len(imgs) > 0:\n",
    "            image_paths.append(imgs[0])\n",
    "        else:\n",
    "            image_paths.append(None)\n",
    "\n",
    "    pokedex[\"image_path\"] = image_paths\n",
    "    pokedex = pokedex[pokedex[\"image_path\"].notna()].reset_index(drop=True)\n",
    "\n",
    "    # only keep pokemon with distinct id\n",
    "    ids, id_counts = np.unique(pokedex[\"ID\"], return_counts=True)\n",
    "    ids, id_counts = np.array(ids), np.array(id_counts)\n",
    "    keep_ids = ids[id_counts == 1]\n",
    "\n",
    "    pokedex = pokedex[pokedex[\"ID\"].isin(keep_ids)].reset_index(drop=True)\n",
    "    pokedex[\"Type2\"] = pokedex[\"Type2\"].str.strip()\n",
    "    if type_to_load is not None:\n",
    "        pokedex = pokedex[pokedex[\"Type1\"].isin(type_to_load)].reset_index(drop=True)\n",
    "    return pokedex\n",
    "\n",
    "# load clip model\n",
    "def load_clip_model():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "    return model, preprocess, device\n",
    "\n",
    "# inference clip model on a list of image path\n",
    "def clip_inference_image(model, preprocess, image_paths, device):\n",
    "    image_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for img_path in tqdm(image_paths):\n",
    "            img = Image.open(img_path)\n",
    "            img_preprocessed = preprocess(img).unsqueeze(0).to(device)\n",
    "            image_embedding = model.encode_image(img_preprocessed).detach().cpu().numpy()\n",
    "            image_embeddings += [image_embedding]\n",
    "            \n",
    "    image_embeddings = np.concatenate(image_embeddings, axis=0)\n",
    "    image_embeddings /= np.linalg.norm(image_embeddings, axis=-1, keepdims=True)\n",
    "    return image_embeddings\n",
    "\n",
    "# inference clip model on a list of texts\n",
    "def clip_inference_text(model, preprocess, texts, device):\n",
    "    with torch.no_grad():\n",
    "        text_embeddings = model.encode_text(clip.tokenize(texts).to(device)).detach().cpu().numpy()\n",
    "    text_embeddings /= np.linalg.norm(text_embeddings, axis=-1, keepdims=True)\n",
    "    return text_embeddings\n",
    "\n",
    "# compute similarity of texts to each image\n",
    "def compute_similarity_text_to_image(image_embeddings, text_embeddings):\n",
    "    similarity = softmax((100.0 * image_embeddings @ text_embeddings.T), axis=-1)\n",
    "    return similarity\n",
    "\n",
    "# compute similarity of iamges to each text\n",
    "def compute_similarity_image_to_text(image_embeddings, text_embeddings):\n",
    "    similarity = softmax((100.0 * image_embeddings @ text_embeddings.T), axis=0)\n",
    "    return similarity\n",
    "\n",
    "# Use TSNE to project CLIP embeddings to 2D space\n",
    "def umap_projection(image_embeddings, n_neighbors=15, min_dist=0.1, metric='cosine'):\n",
    "    distance_matrix = np.zeros((image_embeddings.shape[0], image_embeddings.shape[0]))\n",
    "    for i in range(image_embeddings.shape[0]):\n",
    "        for j in range(image_embeddings.shape[0]):\n",
    "            if i == j:\n",
    "                distance_matrix[i, j] = 1\n",
    "            else:\n",
    "                distance_matrix[i, j] = np.dot(image_embeddings[i], image_embeddings[j])\n",
    "    distance_matrix = 1 - distance_matrix\n",
    "    reducer = TSNE(n_components=2, metric=\"precomputed\", init=\"random\", random_state=42)\n",
    "    visualization_data = reducer.fit_transform(distance_matrix)\n",
    "    return visualization_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "219",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
