{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/chandlerbeon/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/chandlerbeon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/chandlerbeon/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/chandlerbeon/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/chandlerbeon/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>summary</th>\n",
       "      <th>keywords</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>authors</th>\n",
       "      <th>url</th>\n",
       "      <th>leaf_label</th>\n",
       "      <th>root_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Personalize Your NBA App Experience for the '...</td>\n",
       "      <td>'Personalize Your NBA App Experience for the '...</td>\n",
       "      <td>['original', 'content', 'live', 'slate', 'game...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Official Release']</td>\n",
       "      <td>https://www.nba.com/news/nba-app-new-features-...</td>\n",
       "      <td>basketball</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Mike Will attends the Pre-GRAMMY Gala and GRA...</td>\n",
       "      <td>'Mike WiLL Made-It has secured a partnership w...</td>\n",
       "      <td>['lead', 'espn', 'nbas', 'madeit', 'nba', 'lat...</td>\n",
       "      <td>2023-10-18 16:22:29+00:00</td>\n",
       "      <td>['Marc Griffin']</td>\n",
       "      <td>https://www.vibe.com/news/entertainment/mike-w...</td>\n",
       "      <td>basketball</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'The Golden State Warriors are struggling to f...</td>\n",
       "      <td>'The Golden State Warriors are struggling to f...</td>\n",
       "      <td>['insider', 'york', 'thing', 'nbc', 'tag', 'nb...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://www.nbcnewyork.com/tag/featured-nba/</td>\n",
       "      <td>basketball</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'On Nov. 28, the NBA and Nike will collaborate...</td>\n",
       "      <td>'On Nov. 28, the NBA and Nike will collaborate...</td>\n",
       "      <td>['watch', 'telecast', 'ultimate', 'membership'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Official Release']</td>\n",
       "      <td>https://www.nba.com/news/watch-nba-games-ultim...</td>\n",
       "      <td>basketball</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'The NBA announced additions and innovations t...</td>\n",
       "      <td>'The NBA announced additions and innovations t...</td>\n",
       "      <td>['experience', 'bring', 'media', 'crennan', 'n...</td>\n",
       "      <td>2023-10-17 12:00:17+00:00</td>\n",
       "      <td>['Chris Novak', 'About Chris Novak']</td>\n",
       "      <td>https://awfulannouncing.com/tech/nba-app-2023-...</td>\n",
       "      <td>basketball</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           full_text  \\\n",
       "0  'Personalize Your NBA App Experience for the '...   \n",
       "1  'Mike Will attends the Pre-GRAMMY Gala and GRA...   \n",
       "2  'The Golden State Warriors are struggling to f...   \n",
       "3  'On Nov. 28, the NBA and Nike will collaborate...   \n",
       "4  'The NBA announced additions and innovations t...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  'Personalize Your NBA App Experience for the '...   \n",
       "1  'Mike WiLL Made-It has secured a partnership w...   \n",
       "2  'The Golden State Warriors are struggling to f...   \n",
       "3  'On Nov. 28, the NBA and Nike will collaborate...   \n",
       "4  'The NBA announced additions and innovations t...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  ['original', 'content', 'live', 'slate', 'game...   \n",
       "1  ['lead', 'espn', 'nbas', 'madeit', 'nba', 'lat...   \n",
       "2  ['insider', 'york', 'thing', 'nbc', 'tag', 'nb...   \n",
       "3  ['watch', 'telecast', 'ultimate', 'membership'...   \n",
       "4  ['experience', 'bring', 'media', 'crennan', 'n...   \n",
       "\n",
       "                publish_date                               authors  \\\n",
       "0                        NaN                  ['Official Release']   \n",
       "1  2023-10-18 16:22:29+00:00                      ['Marc Griffin']   \n",
       "2                        NaN                                    []   \n",
       "3                        NaN                  ['Official Release']   \n",
       "4  2023-10-17 12:00:17+00:00  ['Chris Novak', 'About Chris Novak']   \n",
       "\n",
       "                                                 url  leaf_label root_label  \n",
       "0  https://www.nba.com/news/nba-app-new-features-...  basketball     sports  \n",
       "1  https://www.vibe.com/news/entertainment/mike-w...  basketball     sports  \n",
       "2       https://www.nbcnewyork.com/tag/featured-nba/  basketball     sports  \n",
       "3  https://www.nba.com/news/watch-nba-games-ultim...  basketball     sports  \n",
       "4  https://awfulannouncing.com/tech/nba-app-2023-...  basketball     sports  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in dataset\n",
    "df = pd.read_csv(\"data/Project1-ClassificationDataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Split dataset\n",
    "train, test = train_test_split(df[[\"full_text\",\"root_label\"]], test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing & Feature Extraction using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given function to parse out HTML-related characters\n",
    "def clean(text):\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    texter = re.sub(r\"<br />\", \" \", text)\n",
    "    texter = re.sub(r\"&quot;\", \"\\\"\",texter)\n",
    "    texter = re.sub('&#39;', \"\\\"\", texter)\n",
    "    texter = re.sub('\\n', \" \", texter)\n",
    "    texter = re.sub(' u ',\" you \", texter)\n",
    "    texter = re.sub('`',\"\", texter)\n",
    "    texter = re.sub(' +', ' ', texter)\n",
    "    texter = re.sub(r\"(!)\\1+\", r\"!\", texter)\n",
    "    texter = re.sub(r\"(\\?)\\1+\", r\"?\", texter)\n",
    "    texter = re.sub('&amp;', 'and', texter)\n",
    "    texter = re.sub('\\r', ' ',texter)\n",
    "    clean = re.compile('<.*?>')\n",
    "    texter = texter.encode('ascii', 'ignore').decode('ascii')\n",
    "    texter = re.sub(clean, '', texter)\n",
    "    if texter == \"\":\n",
    "        texter = \"\"\n",
    "    return texter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom transformer to clean the dataset\n",
    "class DocumentPreprocessingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Strip all HTML-related artifacts\n",
    "        X = X.apply(clean)\n",
    "\n",
    "        # Remove all punctuation and digits\n",
    "        ref_punct_digits = string.punctuation + string.digits\n",
    "        X = X.apply(lambda x: x.translate(str.maketrans('', '', ref_punct_digits)))\n",
    "\n",
    "        # Makes all characters lower-case\n",
    "        # Note: Technically the CountVectorizer already handles it, but is necessary\n",
    "        #       for lemmatization.\n",
    "        X = X.apply(lambda x: x.lower())\n",
    "        return X\n",
    "\n",
    "# Define custom transfomer for lemmiatizatino\n",
    "class LemmatizationPOSTransfomer(BaseEstimator, TransformerMixin):\n",
    "    def lemmatize_documents(self, doc, wnl):\n",
    "        \"\"\"\n",
    "        Lemmatizes documents (i.e. each entry) and returns the lemmatized\n",
    "        document as a single string.\n",
    "        \"\"\"\n",
    "        return ' '.join([wnl.lemmatize(w[0], self.get_wordnet_pos(w[1])) for w in pos_tag(word_tokenize(doc))])\n",
    "\n",
    "\n",
    "    # Nested function for getting part of speech\n",
    "    def get_wordnet_pos(self, tag):\n",
    "        \"\"\"\n",
    "        Maps POS tags to WordNet POS tags.\n",
    "\n",
    "        Default to Noun.\n",
    "        \"\"\"\n",
    "        if tag.startswith('J'):  # Adjective\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):  # Verb\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):  # Noun\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):  # Adverb\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        wnl = WordNetLemmatizer()\n",
    "        X = X.apply(lambda x: self.lemmatize_documents(x, wnl))\n",
    "        return X\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    # Step 1: Data Cleaning\n",
    "    ('doc_preprocess', DocumentPreprocessingTransformer()),\n",
    "    # Step 2: Lemmatization\n",
    "    ('lemmatization', LemmatizationPOSTransfomer()),\n",
    "    # Step 3: Count Vectorization + Stop Word Removal\n",
    "    ('cvector', CountVectorizer(stop_words='english', min_df=3)),\n",
    "    # Step 4: TF-IDF Transformation\n",
    "    ('tfidf', TfidfTransformer())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 412591 stored elements and shape (2780, 13795)>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_processed = pipe.fit_transform(train['full_text'])\n",
    "tfidf_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece219_proj1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
